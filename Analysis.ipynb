{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnalysis on how the ownership of a token holder change\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Analysis on how the ownership of a token holder change\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "#disable the annoying security warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "import re\n",
    "import pandas\n",
    "import numpy\n",
    "from time import sleep\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page 1 of owner0xdb7b85f792e7ff56abe78b03338ed24fbd088464\n",
      "1.46151280403 second for each request\n",
      "processing page 1 of owner0xa3da370e7e81fd7889a0c3f050c46646a9d6a2ab\n",
      "1.71352601051 second for each request\n",
      "processing page 1 of owner0x91136d2250fef45d08d3e34bf9e450a7067456d7\n",
      "0.700570106506 second for each request\n",
      "processing page 1 of owner0x9c00c631b36ba5d93ed8f5279a4c55e6c2ba7c66\n",
      "0.470548868179 second for each request\n",
      "processing page 1 of owner0xe3f5ed01e4990f990e1f0eb8d67c364c3c584ce6\n",
      "1.92985892296 second for each request\n",
      "processing page 1 of owner0xa27b29df3664fe34a8d942477a8e1279ba0faca7\n",
      "0.516279935837 second for each request\n",
      "processing page 1 of owner0x6dcd4cbcd6767d798a47a4bfcd0aaec3cba2a301\n",
      "8.62954807281 second for each request\n",
      "processing page 1 of owner0xe7e561fc67d0008e590fd383fc0fe1571fb7b27e\n",
      "2.05141711235 second for each request\n",
      "processing page 1 of owner0xff0cb0351a356ad16987e5809a8daaaf34f5adbe\n",
      "1.62492394447 second for each request\n",
      "processing page 2 of owner0xff0cb0351a356ad16987e5809a8daaaf34f5adbe\n",
      "0.1283390522 second for each request\n",
      "processing page 1 of owner0x1d73fe12a16f0299c1c9a01f324f7f37a34cca45\n",
      "1.41291499138 second for each request\n",
      "processing page 1 of owner0xa6a1fd66f8eb61a4382c578150a48967966bb418\n",
      "0.328139066696 second for each request\n",
      "processing page 1 of owner0xf6387de51ae0ea118c78a76f69966559d990b099\n",
      "0.562796115875 second for each request\n",
      "processing page 1 of owner0x174443351e21d47ed9ab51517a301107d92ede64\n",
      "9.94079899788 second for each request\n",
      "processing page 2 of owner0x174443351e21d47ed9ab51517a301107d92ede64\n",
      "13.4061350822 second for each request\n",
      "processing page 3 of owner0x174443351e21d47ed9ab51517a301107d92ede64\n",
      "10.6679770947 second for each request\n",
      "processing page 4 of owner0x174443351e21d47ed9ab51517a301107d92ede64\n",
      "9.93836402893 second for each request\n",
      "processing page 5 of owner0x174443351e21d47ed9ab51517a301107d92ede64\n"
     ]
    }
   ],
   "source": [
    "#the base url for etherscan\n",
    "baseUrl='https://etherscan.io/'\n",
    "#the connection pool\n",
    "pool=urllib3.PoolManager()\n",
    "def html_convert_top100(tokenid, classname):\n",
    "    '''\n",
    "    Get the top 100 owners of the coin\n",
    "    '''\n",
    "    global baseUrl\n",
    "    global pool\n",
    "    funUrl_chart='token/tokenholderchart/'\n",
    "    r=pool.request('GET',baseUrl+funUrl_chart+tokenid)\n",
    "    html=r.data\n",
    "    soup = BeautifulSoup(html)\n",
    "    table = soup.find(\"table\", attrs=classname)\n",
    "    headings = [th.get_text() for th in table.find(\"tr\").find_all(\"th\")]\n",
    "    for idx in range(len(headings)):\n",
    "        headings[idx]=str(headings[idx])\n",
    "    datasets = []\n",
    "    for row in table.find_all(\"tr\")[1:]:\n",
    "        dataset = (td.get_text() for td in row.find_all(\"td\"))\n",
    "        ls=list(dataset)\n",
    "        datasets.append(ls)\n",
    "\n",
    "    '''\n",
    "    Clean the data\n",
    "    '''\n",
    "    for idx in range(len(datasets)):\n",
    "        for idxx in range(len(datasets[idx])):\n",
    "            tmp=re.sub(r'\\([^)]*\\)', '', datasets[idx][idxx])\n",
    "            tmp=tmp.strip()\n",
    "            tmp=str(tmp)\n",
    "            datasets[idx][idxx]=tmp\n",
    "\n",
    "    '''\n",
    "    Create pandas dataframe and convert it to float\n",
    "    '''\n",
    "    df=pandas.DataFrame(datasets, columns=headings)\n",
    "    df['Quantity (Token)']=df['Quantity (Token)'].str.replace(',','')\n",
    "    df['Quantity (Token)']=df['Quantity (Token)'].astype(numpy.float64)\n",
    "    df['Percentage']=df['Percentage'].str.strip('%')\n",
    "    df['Percentage']=df['Percentage'].astype(numpy.float64)/100\n",
    "    return df\n",
    "\n",
    "def owners_tr(ownerid, tokenname, classname):\n",
    "    global baseUrl\n",
    "    global pool\n",
    "    transUrl='tokentxns?a='\n",
    "    nextlinks=[]\n",
    "    nextlinks.append(transUrl+ownerid)\n",
    "    '''\n",
    "    trans_dic={\n",
    "        tx:{\n",
    "            Value:'', \n",
    "            Block:''\n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    i=1\n",
    "    while len(nextlinks)>0:\n",
    "        starttime=time.time()\n",
    "        trans_dic={}\n",
    "        print \"processing page \"+str(i)+\" of owner\"+ownerid\n",
    "        link=nextlinks.pop()\n",
    "        r=pool.request('GET',baseUrl+link)\n",
    "        html=r.data\n",
    "        soup = BeautifulSoup(html)\n",
    "        '''\n",
    "        Get the next link\n",
    "        '''\n",
    "        a_tag=soup.find_all('a', id=\"ContentPlaceHolder1_HyperLinkNext\",href=True)\n",
    "        if a_tag[0]['href']!='#':\n",
    "            next_link=a_tag[0]['href']\n",
    "            nextlinks.append(next_link)\n",
    "        table = soup.find(\"table\", attrs=classname)\n",
    "        headings = [th.get_text() for th in table.find(\"tr\").find_all(\"th\")]\n",
    "        for idx in range(len(headings)):\n",
    "            headings[idx]=str(headings[idx])\n",
    "            if headings[idx]=='':\n",
    "                headings[idx]='direction'\n",
    "        datasets = []\n",
    "        for row in table.find_all(\"tr\")[1:]:\n",
    "            dataset = (td.get_text() for td in row.find_all(\"td\"))\n",
    "            ls=list(dataset)\n",
    "            datasets.append(ls)\n",
    "        '''\n",
    "        Clean the data\n",
    "        '''\n",
    "        for idx in range(len(datasets)):\n",
    "            for idxx in range(len(datasets[idx])):\n",
    "                tmp=re.sub(r'\\([^)]*\\)', '', datasets[idx][idxx])\n",
    "                tmp=tmp.strip()\n",
    "                tmp=str(tmp)\n",
    "                datasets[idx][idxx]=tmp\n",
    "        '''\n",
    "        Create pandas dataframe and convert it to float\n",
    "        '''\n",
    "        df=pandas.DataFrame(datasets, columns=headings)\n",
    "        df['Token']=df['Token'].str.upper()\n",
    "        df=df.loc[df['Token'] == tokenname]\n",
    "        df['Value']=df['Value'].str.replace(',','')\n",
    "        df['Value']=df['Value'].astype(numpy.float64)\n",
    "        for index, row in df.iterrows():\n",
    "            if row['Value']!=0:\n",
    "                if row['direction'] == 'OUT':\n",
    "                    val=-row['Value']\n",
    "                else:\n",
    "                    val=row['Value']\n",
    "                trans_dic[row['TxHash']]={}\n",
    "                trans_dic[row['TxHash']]['Value']=val\n",
    "        \n",
    "        txurl='tx/'\n",
    "        for tx in trans_dic:\n",
    "            block=''\n",
    "            req=pool.request('GET',baseUrl+txurl+tx)\n",
    "            html_tx=req.data\n",
    "            tx_soup=BeautifulSoup(html_tx)\n",
    "            tx_a_tag=tx_soup.find_all('a',href=True)\n",
    "            for tag in tx_a_tag:\n",
    "                if '/block/' in tag['href']:\n",
    "                    block=str(tag.getText())\n",
    "                    trans_dic[tx]['Block']=block\n",
    "        i=i+1\n",
    "        elapsed=time.time()-starttime\n",
    "        print str(elapsed)+\" second for each request\"\n",
    "    return trans_dic\n",
    "\n",
    "def ICO_TOKEN(tokenid, tokenname):\n",
    "\n",
    "    df=html_convert_top100(tokenid, \"table table-hover \")\n",
    "    '''\n",
    "    Construct the ownership table\n",
    "    '''\n",
    "    owners={}\n",
    "    for index, row in df.iterrows():\n",
    "        owners[row['Address']]=row['Quantity (Token)']\n",
    "    '''\n",
    "    Construct the ownership transaction table\n",
    "    '''\n",
    "    trans_history={}\n",
    "    for owner in owners:\n",
    "        trans_history[owner]=owners_tr(owner,tokenname, 'table table-hover ')\n",
    "    \n",
    "    '''\n",
    "    Backout the transaction history\n",
    "    '''\n",
    "    headtable=['Block Height', 'Owner', 'TransactionID', 'TOKEN', 'BALANCE']\n",
    "    content=[]\n",
    "    o=1\n",
    "    for owner in owners:\n",
    "        print \"processing owner \"+str(o)+\" there are total \"+len(owners)+\" owners\"\n",
    "        balance=owners[owner]\n",
    "        trans=trans_history[owner]\n",
    "        for t in trans:\n",
    "            entry=[]\n",
    "            #transaction ID\n",
    "            TID=t\n",
    "            #Block Height\n",
    "            Block=trans[t]['Block']\n",
    "            balance=balance-trans[t]['Value']\n",
    "            entry.append(Block)\n",
    "            entry.append(owner)\n",
    "            entry.append(TID)\n",
    "            entry.append(tokenname)\n",
    "            entry.append(balance)\n",
    "            content.append(entry)\n",
    "        o=o+1\n",
    "    dataframe=pandas.DataFrame(content, columns=headtable)\n",
    "    return dataframe\n",
    "    \n",
    "datahistory=ICO_TOKEN('0x86fa049857e0209aa7d9e616f7eb3b3b78ecfdb0','EOS')\n",
    "datahistory.to_csv('top100.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
